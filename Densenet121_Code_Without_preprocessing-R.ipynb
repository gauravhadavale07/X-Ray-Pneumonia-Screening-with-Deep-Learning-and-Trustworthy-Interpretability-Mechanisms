{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>What This Notebook Does</h1>\n",
        "\n",
        "<p>\n",
        "This notebook performs <strong>end-to-end training and testing of a DenseNet-121 deep learning model for Chest X-Ray Pneumonia Classification</strong>.\n",
        "</p>\n",
        "\n",
        "<p>The workflow includes:</p>\n",
        "\n",
        "<ul>\n",
        "  <li>Custom train/validation split from the dataset</li>\n",
        "  <li>Extensive data augmentation for better generalization</li>\n",
        "  <li>Transfer learning with fine-tuning of the final DenseNet block</li>\n",
        "  <li>MixUp regularization to reduce overfitting</li>\n",
        "  <li>One-Cycle Learning Rate Policy for efficient convergence</li>\n",
        "  <li>Early stopping based on validation AUC</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "During training, the <strong>best model is saved automatically based on highest validation AUC</strong>.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "At the end of the notebook, a complete evaluation on the test set is performed, including:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>AUC</li>\n",
        "  <li>F1-score</li>\n",
        "  <li>Classification report</li>\n",
        "  <li>Confusion matrix using the optimal threshold derived from validation performance</li>\n",
        "</ul>\n"
      ],
      "metadata": {
        "id": "s5QPRNRRM7X-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Necessary Libraries +Device Setup\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L9TyeQKaMiz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "8pkmzQEFNYS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Drive + Unzip Dataset"
      ],
      "metadata": {
        "id": "I5M5s16PNY2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/X_ray_images/trust.zip\" -d \"/content/\"\n"
      ],
      "metadata": {
        "id": "7bUxKTCyNjXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Directory Setup + Train/Validation Split\n",
        "\n",
        "---\n",
        "Since the original dataset had only about abt 48 images for testing and validation set folder,I added more images in the new_test and val section for better intrepretation\n"
      ],
      "metadata": {
        "id": "hp56IFOjNmWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig_train = \"/content/chest_xray/train\"\n",
        "test_dir = \"/content/chest_xray/test\"\n",
        "\n",
        "new_train = \"/content/chest_xray/new_train\"\n",
        "new_val = \"/content/chest_xray/new_val\"\n",
        "\n",
        "for d in [new_train, new_val]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    os.makedirs(os.path.join(d, \"NORMAL\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(d, \"PNEUMONIA\"), exist_ok=True)\n",
        "\n",
        "normal_imgs = [os.path.join(orig_train, \"NORMAL\", f) for f in os.listdir(os.path.join(orig_train, \"NORMAL\"))]\n",
        "pneu_imgs = [os.path.join(orig_train, \"PNEUMONIA\", f) for f in os.listdir(os.path.join(orig_train, \"PNEUMONIA\"))]\n",
        "\n",
        "train_norm, val_norm = train_test_split(normal_imgs, test_size=0.15, random_state=42)\n",
        "train_pneu, val_pneu = train_test_split(pneu_imgs, test_size=0.15, random_state=42)\n",
        "\n",
        "def copy_list(files, dest):\n",
        "    for f in files:\n",
        "        shutil.copy(f, dest)\n",
        "\n",
        "copy_list(train_norm, os.path.join(new_train, \"NORMAL\"))\n",
        "copy_list(val_norm, os.path.join(new_val, \"NORMAL\"))\n",
        "copy_list(train_pneu, os.path.join(new_train, \"PNEUMONIA\"))\n",
        "copy_list(val_pneu, os.path.join(new_val, \"PNEUMONIA\"))\n",
        "\n",
        "print(\"New train/val split complete.\")\n"
      ],
      "metadata": {
        "id": "JmNiB1yTNoBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Transforms"
      ],
      "metadata": {
        "id": "5BJHtgf4OcrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 224\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.RandomAffine(10, translate=(0.02, 0.02)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "xiCumA4rOf3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasets + Dataloaders"
      ],
      "metadata": {
        "id": "jkUjhK_JOlFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = datasets.ImageFolder(new_train, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(new_val, transform=test_tfms)\n",
        "test_ds  = datasets.ImageFolder(test_dir, transform=test_tfms)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=0)\n"
      ],
      "metadata": {
        "id": "8IhcLS26OkKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Setup-DENSENET121"
      ],
      "metadata": {
        "id": "P0r6BzKsOquY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.densenet121(weights=\"IMAGENET1K_V1\")\n",
        "numf = model.classifier.in_features\n",
        "model.classifier = nn.Linear(numf, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    if \"denseblock4\" in name or \"norm5\" in name or \"classifier\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4,\n",
        "    weight_decay=1e-4\n",
        ")\n",
        "\n",
        "scheduler = None\n"
      ],
      "metadata": {
        "id": "5H0uqxaROroF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mixup + Training Functions"
      ],
      "metadata": {
        "id": "sCWXDY0OOtnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup_data(x, y, alpha=0.2):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0)).to(device)\n",
        "    mixed_x = lam * x + (1-lam) * x[idx]\n",
        "    return mixed_x, y, y[idx], lam\n",
        "\n",
        "def train_epoch():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, labels in tqdm(train_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.float().unsqueeze(1).to(device)\n",
        "\n",
        "        xmix, ya, yb, lam = mixup_data(imgs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xmix)\n",
        "\n",
        "        loss = lam * criterion(out, ya) + (1-lam) * criterion(out, yb)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(loader):\n",
        "    model.eval()\n",
        "    probs, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            out = model(imgs)\n",
        "            p = torch.sigmoid(out).cpu().numpy()\n",
        "            probs.extend(p)\n",
        "            trues.extend(labels.numpy())\n",
        "    probs = np.array(probs).flatten()\n",
        "    trues = np.array(trues)\n",
        "    auc = roc_auc_score(trues, probs)\n",
        "    preds = (probs > 0.5).astype(int)\n",
        "    f1 = f1_score(trues, preds)\n",
        "    return auc, f1\n"
      ],
      "metadata": {
        "id": "im2CF_9iOwG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop & Saving Best Model"
      ],
      "metadata": {
        "id": "FHrBQlGhOyMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"/content/saved_models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "best_auc = 0\n",
        "patience, es = 5, 0\n",
        "EPOCHS = 25\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-4,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "best_path = f\"{save_dir}/best_model.pth\"\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "\n",
        "    train_loss = train_epoch()\n",
        "    val_auc, val_f1 = evaluate(val_loader)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val AUC: {val_auc:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"Saved BEST model â†’ {best_path}\")\n",
        "        es = 0\n",
        "    else:\n",
        "        es += 1\n",
        "        print(f\"No improvement ({es}/{patience})\")\n",
        "\n",
        "    if es >= patience:\n",
        "        print(\"Early stopping.\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "i-9VZ69COz5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Final Model"
      ],
      "metadata": {
        "id": "09O7z2u9O4JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_path = f\"{save_dir}/final_model.pth\"\n",
        "torch.save(model.state_dict(), final_path)\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(\"Best model:\", best_path)\n",
        "print(\"Final model:\", final_path)\n"
      ],
      "metadata": {
        "id": "NqJeiVKYO8ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Set Evaluation"
      ],
      "metadata": {
        "id": "4Zz585FFPYXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# BLOCK: TEST SET EVALUATION\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def get_preds(model, loader):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    trues = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            p = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "            probs.extend(p)\n",
        "            trues.extend(labels.numpy())\n",
        "\n",
        "    return np.array(probs).flatten(), np.array(trues)\n",
        "\n",
        "\n",
        "print(\"\\n Running TEST evaluation...\")\n",
        "\n",
        "test_probs, test_labels = get_preds(model, test_loader)\n",
        "\n",
        "test_auc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "bin_preds = (test_probs > 0.5).astype(int)\n",
        "test_f1 = f1_score(test_labels, bin_preds)\n",
        "\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(f\"Test ROC AUC: {test_auc:.4f}\")\n",
        "print(f\"Test F1 Score (0.5 threshold): {test_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "-4RG_CXePWvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finding best threshold on validation set by running threshold sweep"
      ],
      "metadata": {
        "id": "HemEtGZpPwMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n Running threshold sweep on VALIDATION SET...\")\n",
        "\n",
        "val_probs, val_labels = get_preds(model, val_loader)\n",
        "\n",
        "thresholds = np.linspace(0, 1, 400)\n",
        "best_f1 = 0\n",
        "best_thresh = 0\n",
        "\n",
        "for th in thresholds:\n",
        "    preds = (val_probs > th).astype(int)\n",
        "    f1 = f1_score(val_labels, preds)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = th\n",
        "\n",
        "print(\"\\n Best Threshold =\", round(best_thresh, 4))\n",
        "print(\" Best F1 on VAL =\", round(best_f1, 4))\n"
      ],
      "metadata": {
        "id": "kiOCgJgDPwpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Metrics"
      ],
      "metadata": {
        "id": "WO2cIykIQFT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "test_preds_best = (test_probs > best_thresh).astype(int)\n",
        "\n",
        "print(\"\\nCONFUSION MATRIX (TEST SET)\")\n",
        "cm = confusion_matrix(test_labels, test_preds_best)\n",
        "print(cm)\n",
        "\n",
        "print(\"\\n CLASSIFICATION REPORT\")\n",
        "print(classification_report(test_labels, test_preds_best, target_names=[\"NORMAL\", \"PNEUMONIA\"]))\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(cm, cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Test Set)\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0,1], [\"NORMAL\",\"PNEUMONIA\"])\n",
        "plt.yticks([0,1], [\"NORMAL\",\"PNEUMONIA\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0_FIXZGuQEgi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}